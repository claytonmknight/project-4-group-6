{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"FBk6_tLolG1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install PySpark, FindSpark, and Keras Tuner\n","!pip install pyspark\n","!pip install -q findspark\n","!pip install keras-tuner"],"metadata":{"id":"L-Z0SYcThsnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCFbuBvXPWJ8"},"outputs":[],"source":["# Standard Libraries\n","import os\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","# Spark and PySpark\n","import findspark\n","from pyspark.sql import SparkSession\n","from pyspark import SparkFiles\n","\n","# Scikit-learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","\n","# Imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","# TensorFlow and Keras\n","import tensorflow as tf\n","from keras import callbacks\n","import keras_tuner as kt\n","\n","# Setting up Spark version\n","spark_version = 'spark-3.5.1'\n","os.environ['SPARK_VERSION'] = spark_version"]},{"cell_type":"markdown","source":["Spark Session"],"metadata":{"id":"eZoPespPdDTK"}},{"cell_type":"code","source":["# Initialize a Spark session\n","spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n","\n","# Start a SparkSession\n","findspark.init()\n","\n","# Read the CSV file from the Resources folder\n","path = \"/content/drive/MyDrive/project-4-group-6/Resources/bank-full.csv\"\n","spark.sparkContext.addFile(path)\n","df = spark.read.csv(SparkFiles.get(\"bank-full.csv\"), sep=\";\", header=True, inferSchema=True)\n","\n","# Review the DataFrame\n","df.show()"],"metadata":{"id":"_4J2ixZETeBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Register the DataFrame as a SQL temporary view\n","df.createOrReplaceTempView(\"bank_data\")\n","\n","# Run the SQL query to find 35 year olds with \"yes\" in deposit_made\n","print(\"Query to retrieve 35-year-olds who made a deposit:\")\n","query_1 = \"\"\"\n","SELECT *\n","FROM bank_data\n","WHERE age = 35 AND y = 'yes'\n","LIMIT 5\n","\"\"\"\n","spark.sql(query_1).show()\n","\n","# Run SQL query to count occurrences of \"yes\" in deposit_made column by campaign\n","print(\"\\nQuery to count 'yes' occurrences by campaign:\")\n","query_2 = \"\"\"\n","SELECT campaign,\n","COUNT(*)\n","FROM bank_data\n","WHERE y = 'yes'\n","GROUP BY campaign\n","ORDER BY campaign asc\n","\"\"\"\n","spark.sql(query_2).show()\n","\n","# Run SQL query to count occurrences of \"yes\" in deposit_made column by job, sorted by count descending\n","print(\"\\nQuery to count 'yes' occurrences by job, sorted by count descending:\")\n","query_3 = \"\"\"\n","SELECT job,\n","COUNT(*)\n","FROM bank_data\n","WHERE y = 'yes'\n","GROUP BY job\n","ORDER BY count(job) desc\n","\"\"\"\n","spark.sql(query_3).show()"],"metadata":{"id":"xqZ3Y1BpTo-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pandas"],"metadata":{"id":"n-NltX_pgbG4"}},{"cell_type":"code","source":["# Convert Spark DF to Pandas\n","bank_full_df = df.toPandas()\n","\n","# Rename the column 'y' to 'deposit_made'\n","bank_full_df.rename(columns={'y': 'deposit_made'}, inplace=True)\n","\n","# Convert deposit_made from categorical to numeric for later clustering purposes\n","bank_full_df.replace(\"no\", 0, inplace=True)\n","bank_full_df.replace(\"yes\", 1, inplace=True)\n","\n","# Review the DataFrame\n","bank_full_df.head(-20)"],"metadata":{"id":"TZzLrvvC3St0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Figures"],"metadata":{"id":"0P_F-ffGdKDx"}},{"cell_type":"code","source":["# Create a figure with subplots\n","fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","# Group by 'age' and count 'deposit_made' = 'yes',\n","age_deposit_counts = bank_full_df[bank_full_df['deposit_made'] == 1].groupby('age').size()\n","\n","# Plot the data on the first subplot\n","age_deposit_counts.plot(kind='line', color='g', ax=axes[0, 0])\n","axes[0, 0].set_ylabel('Count of Deposit Made')\n","axes[0, 0].set_xlabel('Age')\n","axes[0, 0].set_title('Deposits Made by Age')\n","\n","# Group by 'job' and count 'deposit_made' = 'yes', then sort by count in ascending order\n","Job_deposit_counts = bank_full_df[bank_full_df['deposit_made'] == 1].groupby('job').size().sort_values(ascending=True)\n","\n","# Plot the sorted data on the second subplot\n","Job_deposit_counts.plot(kind='barh', color='b', ax=axes[0, 1])\n","axes[0, 1].set_xlabel('Count of Deposit Made')\n","axes[0, 1].set_ylabel('Job')\n","axes[0, 1].set_title('Deposits Made by Job Category')\n","\n","# Group by 'job' and 'deposit_made', count occurrences\n","Job_total_deposit_counts = bank_full_df.groupby(['job', 'deposit_made']).size().unstack()\n","Job_total_deposit_counts = Job_total_deposit_counts.sort_values(by=1, ascending=True)\n","\n","# Plot the sorted data on the third subplot\n","Job_total_deposit_counts.plot(kind='barh', color=('r','b'), ax=axes[1, 0])\n","axes[1, 0].set_title('Job Categories vs Deposits Made')\n","axes[1, 0].set_xlabel('Number of Deposits')\n","axes[1, 0].set_ylabel('Job')\n","\n","# Group by 'campaign' and count 'deposit_made' = 'yes'\n","contacts_deposits = bank_full_df[bank_full_df['deposit_made'] == 1].groupby('campaign').size()\n","\n","# Plot the data with a grid on the fourth subplot\n","contacts_deposits.plot(kind='area', color='b', grid=True, ax=axes[1, 1])\n","axes[1, 1].set_ylabel('Number of Customers who made Deposits')\n","axes[1, 1].set_xlabel('Number of Times Customer was Contacted before deposit was made')\n","axes[1, 1].set_title('Deposits Made by Number of Campaign Contacts')\n","\n","# Adjust layout\n","plt.tight_layout()\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"43bAUHs3oOR6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre-processing"],"metadata":{"id":"P7AP0fzQgqOO"}},{"cell_type":"code","source":["# Separate the y variable\n","y = bank_full_df['deposit_made']\n","\n","# Separate the X variable\n","X = bank_full_df.drop(columns=['deposit_made'])\n","\n","# Review the y variable Series\n","print(\"y variable Series:\")\n","print(y.head())"],"metadata":{"id":"TpV74q-l3Br-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Review the X variable DataFrame\n","print(\"\\nX variable DataFrame:\")\n","print(X.head())"],"metadata":{"id":"unHGcb9s3rd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert categorical values to numeric\n","X = pd.get_dummies(X, dtype=int)\n","\n","# Display the first few rows of the transformed dataset\n","X.head()"],"metadata":{"id":"-LzP5TriMm_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluations"],"metadata":{"id":"56tLYhqlg_E2"}},{"cell_type":"code","source":["# Assign a random_state of 13 to the function and split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13, stratify=y)\n","\n","# Scale testing and training groups\n","scaler = StandardScaler()\n","X_scaler = scaler.fit(X_train)\n","X_train_scaled = X_scaler.transform(X_train)\n","X_test_scaled = X_scaler.transform(X_test)\n","\n","# Create Logistic Regression Model\n","model = LogisticRegression(solver='lbfgs')\n","\n","# Fit the training data to the model\n","model.fit(X_train_scaled, y_train)"],"metadata":{"id":"_NJS5xWY3v3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions\n","predictions = model.predict(X_test_scaled)\n","# Evaluate findings with confusion matrix/classification report\n","report = classification_report(y_test, predictions)\n","print(report)"],"metadata":{"id":"14cwyfYlSOQj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create confusion matrix\n","cm = confusion_matrix(y_test, predictions)\n","\n","# Create a DataFrame to display the confusion matrix\n","cm_df = pd.DataFrame(cm)\n","\n","# Display the confusion matrix DataFrame\n","print(\"Confusion Matrix:\")\n","print(cm_df)"],"metadata":{"id":"gZd9cucCWmEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest Model\n","rf_model = RandomForestClassifier(n_estimators=500, random_state=42)\n","\n","# Train rf_model\n","rf_model.fit(X_train_scaled, y_train)"],"metadata":{"id":"pdAYQFiHSaq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions using the trained Random Forest model on the scaled test data\n","rf_predictions = rf_model.predict(X_test_scaled)\n","\n","# RF classification report\n","rf_report = classification_report(y_test, rf_predictions)\n","print(rf_report)"],"metadata":{"id":"ghcBJsKtUxHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Neural Networks"],"metadata":{"id":"d_sA3NLJdT2C"}},{"cell_type":"code","source":["# Create a method that creates a new Sequential model with hyperparameter options\n","def create_model(hp):\n","    nn_model = tf.keras.models.Sequential()\n","    # Allow kerastuner to decide which activation function to use in hidden layers\n","    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n","    # Allow kerastuner to decide number of neurons in first layer\n","    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n","        min_value=1,\n","        max_value=10,\n","        step=2), activation=activation, input_dim=48))\n","    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n","    for i in range(hp.Int('num_layers', 1, 6)):\n","        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n","            min_value=1,\n","            max_value=10,\n","            step=2),\n","            activation=activation))\n","    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","    # Compile the model\n","    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n","    return nn_model\n","\n","# Define unique file paths for storing tuner configurations\n","tuner1_file_path = './untitled_project/tuner1.json'\n","\n","# Define a Hyperband tuner to search for the best hyperparameters\n","tuner1 = kt.Hyperband(\n","    create_model,\n","    objective=\"val_accuracy\",\n","    max_epochs=20,\n","    hyperband_iterations=2)\n","\n","# Search for the best hyperparameters using the oversampled training data\n","tuner1.search(X_train, y_train, epochs=20, validation_data=(X_test_scaled, y_test))\n","\n","# Get best model hyperparameters\n","best_hyper1 = tuner1.get_best_hyperparameters(1)[0]\n","best_hyper1.values\n","\n","# Evaluate best model against full test data\n","best_model1 = tuner1.get_best_models(1)[0]\n","model_loss1, model_accuracy1 = best_model1.evaluate(X_test_scaled,y_test,verbose=2)\n","print(f\"Loss: {model_loss1}, Accuracy: {model_accuracy1}\")"],"metadata":{"id":"Buv80aeKfOTL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oversampling"],"metadata":{"id":"IceiXTzxf-8b"}},{"cell_type":"code","source":["# Define the desired ratio for oversampling\n","desired_ratio = .5\n","\n","# Calculate the number of samples needed in the minority class to achieve the desired ratio\n","num_majority = y_train.value_counts()[0]\n","num_minority_desired = int(desired_ratio * num_majority)\n","\n","# Apply SMOTE with the specified ratio\n","smote = SMOTE(sampling_strategy={1: num_minority_desired}, random_state=42)\n","\n","# Apply SMOTE to the training data\n","X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_scaled, y_train)\n","\n","# Confirm the new class distribution after oversampling\n","print(\"After oversampling:\")\n","print(y_train_oversampled.value_counts())\n","\n","# Create and train the model on the oversampled data\n","model_smote = LogisticRegression(solver='lbfgs')\n","model_smote.fit(X_train_oversampled, y_train_oversampled)\n","\n","# Make predictions on the test set\n","predictions_smote = model_smote.predict(X_test_scaled)\n","\n","# Evaluate the model with SMOTE\n","report_smote = classification_report(y_test, predictions_smote)\n","print(\"Classification Report with SMOTE:\\n\", report_smote)"],"metadata":{"id":"4cyXzuq1-okj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a method that creates a new Sequential model with hyperparameter options\n","def create_model(hp):\n","    nn_model = tf.keras.models.Sequential()\n","    # Allow kerastuner to decide which activation function to use in hidden layers\n","    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n","    # Allow kerastuner to decide number of neurons in first layer\n","    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n","        min_value=1,\n","        max_value=10,\n","        step=2), activation=activation, input_dim=48))\n","    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n","    for i in range(hp.Int('num_layers', 1, 6)):\n","        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n","            min_value=1,\n","            max_value=10,\n","            step=2),\n","            activation=activation))\n","    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","    # Compile the model\n","    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n","    return nn_model\n","\n","# Define unique file paths for storing tuner configurations\n","tuner2_file_path = './untitled_project/tuner2.json'\n","\n","# Define a Hyperband tuner to search for the best hyperparameters\n","tuner2 = kt.Hyperband(\n","    create_model,\n","    objective=\"val_accuracy\",\n","    max_epochs=20,\n","    hyperband_iterations=2,\n","    directory='./untitled_project',  # Specify directory for tuner2 configuration\n","    project_name='tuner2',  # Specify project name for tuner2 configuration\n",")\n","\n","# Search for the best hyperparameters using the oversampled training data\n","tuner2.search(X_train_oversampled, y_train_oversampled, epochs=20, validation_data=(X_test_scaled, y_test))\n","\n","# Get best model hyperparameters\n","best_hyper2 = tuner2.get_best_hyperparameters(1)[0]\n","best_hyper2.values\n","\n","# Evaluate best model against full test data\n","best_model2 = tuner2.get_best_models(1)[0]\n","model_loss2, model_accuracy2 = best_model2.evaluate(X_test_scaled, y_test, verbose=2)\n","print(f\"Loss: {model_loss2}, Accuracy: {model_accuracy2}\")"],"metadata":{"id":"2bREnshuQVPC"},"execution_count":null,"outputs":[]}]}